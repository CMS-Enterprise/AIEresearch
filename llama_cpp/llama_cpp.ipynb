{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(.venv_dev311) [ec2-user@ip-10-203-107-151 bin]$ sudo yum install gcc-11*\n",
    "Loaded plugins: extras_suggestions, langpacks, priorities, s3iam, update-motd\n",
    "229 packages excluded due to repository priority protections\n",
    "No package gcc-11* available.\n",
    "Error: Nothing to do\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update gcc\n",
    "\n",
    "Resources\n",
    "* https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "* https://github.com/imartinez/privateGPT/issues/644\n",
    "* https://amazonlinux.pkgs.org/2023/amazonlinux-x86_64/gcc-11.3.1-4.amzn2023.0.3.x86_64.rpm.html#:~:text=Download%20gcc%2D11.3.1%2D4.amzn2023.0.3.x86_64.rpm%20for%20Amazon%20Linux%202023%20from%20Amazon%20Linux%20repository.\n",
    "* https://centos.pkgs.org/7/centos-sclo-rh-x86_64/devtoolset-11-gcc-c++-11.2.1-1.2.el7.x86_64.rpm.html\n",
    "* https://pkgs.org/search/?q=Development%20Tools\n",
    "* https://github.com/sclorg/centos-release-scl\n",
    "\n",
    "```\n",
    "\n",
    "#remove old gcc\n",
    "sudo yum remove gcc -y\n",
    "sudo yum remove gdb -y\n",
    "\n",
    "#install scl-utils\n",
    "sudo yum install scl-utils\n",
    "sudo yum install centos-release-scl\n",
    "\n",
    "#find devtoolset-11\n",
    "yum list all --enablerepo='centos-sclo-rh' | grep \"devtoolset\"\n",
    "> NOT WORKING\n",
    "    yum list \\*gcc\\* | grep amzn2 > only shows 10-\n",
    "    yum list all | grep devtoolset\n",
    "    sudo yum groupinstall \"Development Tools\" -y\n",
    "    yum list \\*devtoolset\\*\n",
    "> Attempt\n",
    "cd usr/bin/\n",
    "sudo wget http://mirror.centos.org/centos/7/sclo/x86_64/rh/Packages/d/devtoolset-11-gcc-c++-11.2.1-1.2.el7.x86_64.rpm\n",
    "sudo chmod 755 devtoolset-11-gcc-c++-11.2.1-1.2.el7.x86_64.rpm\n",
    "sudo yum install devtoolset-11-gcc-c++-11.2.1-1.2.el7.x86_64.rpm\n",
    "> Attempt2\n",
    "sudo yum install centos-release-scl-rh\n",
    "> Attempt3 \n",
    "sudo yum-config-manager --enable rhel-server-rhscl-7-rpms\n",
    "sudo yum-config-manager --add-repo=https://copr.fedoraproject.org/coprs/rhscl/centos-release-scl/repo/epel-6/rhscl-centos-release-scl-epel-6.repo\n",
    "sudo yum install centos-release-scl\n",
    "\n",
    "\n",
    "\n",
    "#install devtoolset-11-toolchain\n",
    "sudo yum install devtoolset-11-toolchain -y\n",
    "\n",
    "#add gcc 11 to PATH by adding following script to /etc/profile\n",
    "PATH=$PATH::/opt/rh/devtoolset-11/root/usr/bin export PATH sudo scl enable devtoolset-11 bash\n",
    "\n",
    "#show gcc version and gcc11 is installed successfully.\n",
    "gcc --version\n",
    "\n",
    "# FROM BOTTOM OF THREAD\n",
    "CMAKE_ARGS=\"-DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++\" pip install llama-cpp-python\n",
    "\n",
    "pip install llama-cpp-python==0.1.50```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* https://pypi.org/project/llama-cpp-python/\n",
    "* https://llama-cpp-python.readthedocs.io/en/latest/api-reference/\n",
    "* https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llamacpppython_integration.ipynb\n",
    "* https://github.com/oobabooga/text-generation-webui/issues/1534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> model = llama_cpp.Llama(\n",
    "...     model_path=\"path/to/model\",\n",
    "...     chat_format=\"llama-2\",\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/data/AIEresearch/.venv_dev311/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62db996a175481a91cee4761e98ba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for auto-gptq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdiscover(name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name_or_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTheBloke/Mistral-7B-Instruct-v0.2-code-ft-GPTQ\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# To use a different branch, change revision\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# For example: revision=\"gptq-4bit-32g-actorder_True\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path,\n\u001b[1;32m      7\u001b[0m                                              device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m                                              trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m                                              revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWrite a story about llamas\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/efs/data/AIEresearch/.venv_dev311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/mnt/efs/data/AIEresearch/.venv_dev311/lib/python3.11/site-packages/transformers/modeling_utils.py:3013\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   3005\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou passed `quantization_config` to `from_pretrained` but the model you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre loading already has a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3006\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`quantization_config` attribute and has already quantized weights. However, loading attributes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3007\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (e.g. \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(loading_attr_dict\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3008\u001b[0m     )\n\u001b[1;32m   3009\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3010\u001b[0m     quantization_method_from_args \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   3011\u001b[0m     \u001b[39mor\u001b[39;00m quantization_method_from_config \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   3012\u001b[0m ):\n\u001b[0;32m-> 3013\u001b[0m     gptq_supports_cpu \u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(importlib\u001b[39m.\u001b[39;49mmetadata\u001b[39m.\u001b[39;49mversion(\u001b[39m\"\u001b[39;49m\u001b[39mauto-gptq\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m>\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m0.4.2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3014\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m gptq_supports_cpu \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m   3015\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGPU is required to quantize or run quantize model.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/importlib/metadata/__init__.py:1008\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1002\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[1;32m   1004\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39mversion\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/importlib/metadata/__init__.py:981\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    976\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[1;32m    978\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m Distribution\u001b[39m.\u001b[39;49mfrom_name(distribution_name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/importlib/metadata/__init__.py:565\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdiscover(name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for auto-gptq"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-code-ft-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''/\n",
    "    <|im_start|>system\n",
    "    {system_message}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {prompt}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
